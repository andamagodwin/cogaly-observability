{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§  Cogaly - Early Alzheimer's Risk Detection\n",
    "\n",
    "**AI 4 Alzheimer's Hackathon Submission**\n",
    "\n",
    "This notebook implements an XGBoost classifier with SHAP explainability for early Alzheimer's disease risk detection.\n",
    "\n",
    "## Features\n",
    "- High accuracy (94.42%) on test set\n",
    "- SHAP-based interpretable predictions\n",
    "- Complete training and evaluation pipeline\n",
    "- Production-ready model export\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Packages\n",
    "\n",
    "Run this cell first to install all dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this cell in Google Colab\n",
    "# !pip install pandas numpy scikit-learn xgboost shap matplotlib seaborn -q\n",
    "# print(\"âœ… All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score, precision_score, recall_score,\n",
    "    classification_report, confusion_matrix, roc_curve\n",
    ")\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ðŸ§  COGALY - ALZHEIMER'S RISK DETECTION SYSTEM\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nâœ… All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Dataset\n",
    "\n",
    "**Option 1:** Upload the dataset directly in Colab  \n",
    "**Option 2:** Mount Google Drive  \n",
    "**Option 3:** Use the local path (for local environments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === OPTION 1: Upload file directly in Colab ===\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "# data_path = 'alzheimers.csv'\n",
    "\n",
    "# === OPTION 2: Load from Google Drive ===\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# data_path = '/content/drive/MyDrive/path/to/alzheimers.csv'\n",
    "\n",
    "# === OPTION 3: Local path ===\n",
    "data_path = '../data/alzheimers.csv'  # Change this path as needed\n",
    "\n",
    "print(\"ðŸ“ Data path configured:\", data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸ“Š LOADING DATASET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"âœ… Dataset loaded successfully!\")\n",
    "print(f\"   â€¢ Total samples: {df.shape[0]:,}\")\n",
    "print(f\"   â€¢ Total features: {df.shape[1]}\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ“ˆ Target Distribution:\")\n",
    "target_dist = df['Diagnosis'].value_counts()\n",
    "print(target_dist)\n",
    "print(f\"\\nClass Balance (%):\\n{df['Diagnosis'].value_counts(normalize=True) * 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize target distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "target_dist.plot(kind='bar', color=colors, edgecolor='black')\n",
    "plt.title('Target Distribution: Alzheimer\\'s Diagnosis', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Diagnosis (0 = No Risk, 1 = At Risk)')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "for i, v in enumerate(target_dist):\n",
    "    plt.text(i, v + 20, f'{v}', ha='center', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸ”§ DATA PREPROCESSING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Remove non-predictive columns\n",
    "columns_to_drop = ['PatientID', 'DoctorInCharge']\n",
    "df_processed = df.drop(columns=columns_to_drop, errors='ignore')\n",
    "print(f\"âœ… Removed columns: {columns_to_drop}\")\n",
    "\n",
    "# Separate features and target\n",
    "X = df_processed.drop('Diagnosis', axis=1)\n",
    "y = df_processed['Diagnosis']\n",
    "print(f\"âœ… Features: {X.shape[1]}, Target: Diagnosis\")\n",
    "\n",
    "# Handle missing values\n",
    "missing_count = X.isnull().sum().sum()\n",
    "if missing_count > 0:\n",
    "    print(f\"âš ï¸  Handling {missing_count} missing values...\")\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if X[col].isnull().sum() > 0:\n",
    "            X[col].fillna(X[col].median(), inplace=True)\n",
    "    \n",
    "    categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_cols:\n",
    "        if X[col].isnull().sum() > 0:\n",
    "            X[col].fillna(X[col].mode()[0], inplace=True)\n",
    "    print(f\"âœ… Missing values handled\")\n",
    "else:\n",
    "    print(f\"âœ… No missing values detected\")\n",
    "\n",
    "# Encode categorical features\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "if len(categorical_cols) > 0:\n",
    "    print(f\"âœ… Encoding {len(categorical_cols)} categorical features...\")\n",
    "    X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "print(f\"\\nâœ… Preprocessing complete! Final features: {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train-Test Split (80/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ‚ï¸  SPLITTING DATA (80/20)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"âœ… Training set: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"âœ… Test set: {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸ“ FEATURE SCALING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train), \n",
    "    columns=X_train.columns\n",
    ")\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test), \n",
    "    columns=X_test.columns\n",
    ")\n",
    "\n",
    "print(f\"âœ… Features scaled (mean=0, std=1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Train XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸ¤– TRAINING XGBOOST CLASSIFIER\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "print(\"â³ Training in progress...\")\n",
    "model.fit(X_train_scaled, y_train)\n",
    "print(\"âœ… Model training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸ”® MAKING PREDICTIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred_proba = model.predict_proba(X_test_scaled)\n",
    "\n",
    "print(f\"âœ… Predictions generated for {len(y_test):,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸ“Š MODEL EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nðŸŽ¯ PERFORMANCE METRICS:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"   Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"   ROC-AUC:   {roc_auc:.4f}\")\n",
    "print(f\"   Precision: {precision:.4f} ({precision*100:.2f}%)\")\n",
    "print(f\"   Recall:    {recall:.4f} ({recall*100:.2f}%)\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ“‹ CLASSIFICATION REPORT:\")\n",
    "print(classification_report(y_test, y_pred, \n",
    "                          target_names=['No Alzheimer\\'s', 'Alzheimer\\'s']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f\"\\nðŸ”¢ CONFUSION MATRIX:\")\n",
    "print(f\"   True Negatives:  {cm[0,0]:,}\")\n",
    "print(f\"   False Positives: {cm[0,1]:,}\")\n",
    "print(f\"   False Negatives: {cm[1,0]:,}\")\n",
    "print(f\"   True Positives:  {cm[1,1]:,}\")\n",
    "\n",
    "# Visualize Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "           xticklabels=['No Risk', 'Risk'],\n",
    "           yticklabels=['No Risk', 'Risk'],\n",
    "           annot_kws={'size': 16})\n",
    "plt.title('Confusion Matrix - Cogaly Model', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba[:, 1])\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "        label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', \n",
    "        label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curve - Cogaly Model', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\", fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('roc_curve.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: SHAP Explainability Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸ” SHAP EXPLAINABILITY ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"â³ Computing SHAP values (this may take a moment)...\")\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_test_scaled)\n",
    "print(\"âœ… SHAP values computed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "shap_importance = pd.DataFrame({\n",
    "    'Feature': X_test_scaled.columns,\n",
    "    'Importance': np.abs(shap_values).mean(axis=0)\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(f\"\\nðŸ† TOP 10 CONTRIBUTING FEATURES:\")\n",
    "print(\"-\" * 60)\n",
    "for idx, (_, row) in enumerate(shap_importance.head(10).iterrows(), 1):\n",
    "    print(f\"   {idx:2d}. {row['Feature']:30s} | {row['Importance']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Bar Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values, X_test_scaled, plot_type=\"bar\", show=False)\n",
    "plt.title('SHAP Feature Importance', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('shap_feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Summary Plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "shap.summary_plot(shap_values, X_test_scaled, show=False)\n",
    "plt.title('SHAP Impact on Predictions', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('shap_summary_plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Prediction Function with Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_alzheimer_risk(patient_data, model, scaler, explainer, feature_cols):\n",
    "    \"\"\"\n",
    "    Predict Alzheimer's risk with explainability\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    patient_data : dict or pd.DataFrame\n",
    "        Patient features\n",
    "    model : XGBClassifier\n",
    "        Trained XGBoost model\n",
    "    scaler : StandardScaler\n",
    "        Fitted feature scaler\n",
    "    explainer : shap.TreeExplainer\n",
    "        SHAP explainer for interpretability\n",
    "    feature_cols : list\n",
    "        List of feature names\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Prediction results with explanations\n",
    "    \"\"\"\n",
    "    if isinstance(patient_data, dict):\n",
    "        patient_data = pd.DataFrame([patient_data])\n",
    "    \n",
    "    # Ensure all features present\n",
    "    for col in feature_cols:\n",
    "        if col not in patient_data.columns:\n",
    "            patient_data[col] = 0\n",
    "    \n",
    "    patient_data = patient_data[feature_cols]\n",
    "    \n",
    "    # Scale and predict\n",
    "    patient_scaled = pd.DataFrame(\n",
    "        scaler.transform(patient_data), \n",
    "        columns=feature_cols\n",
    "    )\n",
    "    \n",
    "    prediction = model.predict(patient_scaled)[0]\n",
    "    probability = model.predict_proba(patient_scaled)[0]\n",
    "    \n",
    "    # SHAP values\n",
    "    shap_vals = explainer.shap_values(patient_scaled)\n",
    "    \n",
    "    feature_contrib = pd.DataFrame({\n",
    "        'Feature': feature_cols,\n",
    "        'SHAP_Value': shap_vals[0],\n",
    "        'Value': patient_scaled.values[0]\n",
    "    }).sort_values('SHAP_Value', key=abs, ascending=False)\n",
    "    \n",
    "    top_features = feature_contrib.head(5)\n",
    "    \n",
    "    return {\n",
    "        'predicted_class': int(prediction),\n",
    "        'diagnosis': 'Alzheimer\\'s Risk' if prediction == 1 else 'No Risk',\n",
    "        'risk_score': float(probability[1]),\n",
    "        'confidence': float(max(probability) * 100),\n",
    "        'top_features': [\n",
    "            {\n",
    "                'feature': row['Feature'],\n",
    "                'shap_value': float(row['SHAP_Value']),\n",
    "                'impact': 'Increases Risk' if row['SHAP_Value'] > 0 else 'Decreases Risk'\n",
    "            }\n",
    "            for _, row in top_features.iterrows()\n",
    "        ]\n",
    "    }\n",
    "\n",
    "print(\"âœ… Prediction function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prediction on sample patient\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸ§ª TESTING PREDICTION FUNCTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "sample = X_test.iloc[0:1].copy()\n",
    "result = predict_alzheimer_risk(sample, model, scaler, explainer, X_train.columns.tolist())\n",
    "\n",
    "print(f\"\\nðŸ“‹ SAMPLE PREDICTION:\")\n",
    "print(f\"   Diagnosis: {result['diagnosis']}\")\n",
    "print(f\"   Risk Score: {result['risk_score']:.4f} ({result['risk_score']*100:.2f}%)\")\n",
    "print(f\"   Confidence: {result['confidence']:.2f}%\")\n",
    "print(f\"\\n   Top Contributing Features:\")\n",
    "for i, feat in enumerate(result['top_features'], 1):\n",
    "    print(f\"      {i}. {feat['feature']:25s} | {feat['impact']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Save Model and Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸ’¾ SAVING MODEL AND ARTIFACTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Save model\n",
    "with open('cogaly_xgb_v1.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "print(\"âœ… Model saved: cogaly_xgb_v1.pkl\")\n",
    "\n",
    "# Save scaler\n",
    "with open('cogaly_scaler_v1.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(\"âœ… Scaler saved: cogaly_scaler_v1.pkl\")\n",
    "\n",
    "# Save feature columns\n",
    "with open('cogaly_features_v1.pkl', 'wb') as f:\n",
    "    pickle.dump(X_train.columns.tolist(), f)\n",
    "print(\"âœ… Features saved: cogaly_features_v1.pkl\")\n",
    "\n",
    "# Save explainer\n",
    "with open('cogaly_explainer_v1.pkl', 'wb') as f:\n",
    "    pickle.dump(explainer, f)\n",
    "print(\"âœ… Explainer saved: cogaly_explainer_v1.pkl\")\n",
    "\n",
    "# Save metrics\n",
    "metrics = {\n",
    "    'accuracy': float(accuracy),\n",
    "    'roc_auc': float(roc_auc),\n",
    "    'precision': float(precision),\n",
    "    'recall': float(recall),\n",
    "    'n_features': X_train.shape[1]\n",
    "}\n",
    "\n",
    "with open('cogaly_metrics_v1.pkl', 'wb') as f:\n",
    "    pickle.dump(metrics, f)\n",
    "print(\"âœ… Metrics saved: cogaly_metrics_v1.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Download Files (For Google Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to download files in Google Colab:\n",
    "\n",
    "# from google.colab import files\n",
    "# files.download('cogaly_xgb_v1.pkl')\n",
    "# files.download('cogaly_scaler_v1.pkl')\n",
    "# files.download('cogaly_features_v1.pkl')\n",
    "# files.download('cogaly_explainer_v1.pkl')\n",
    "# files.download('cogaly_metrics_v1.pkl')\n",
    "# files.download('confusion_matrix.png')\n",
    "# files.download('roc_curve.png')\n",
    "# files.download('shap_feature_importance.png')\n",
    "# files.download('shap_summary_plot.png')\n",
    "\n",
    "print(\"\\nâœ… To download in Colab, uncomment the download code above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Training Complete!\n",
    "\n",
    "### Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸŽ‰ COGALY MODEL TRAINING COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\"\"\n",
    "ðŸ“Š MODEL SUMMARY:\n",
    "   â€¢ Algorithm: XGBoost Classifier\n",
    "   â€¢ Features: {X_train.shape[1]}\n",
    "   â€¢ Training Samples: {len(X_train):,}\n",
    "   â€¢ Test Samples: {len(X_test):,}\n",
    "\n",
    "ðŸŽ¯ PERFORMANCE:\n",
    "   â€¢ Accuracy:  {accuracy*100:.2f}%\n",
    "   â€¢ ROC-AUC:   {roc_auc:.4f}\n",
    "   â€¢ Precision: {precision*100:.2f}%\n",
    "   â€¢ Recall:    {recall*100:.2f}%\n",
    "\n",
    "ðŸ’¾ SAVED FILES:\n",
    "   â€¢ cogaly_xgb_v1.pkl (Main model)\n",
    "   â€¢ cogaly_scaler_v1.pkl (Feature scaler)\n",
    "   â€¢ cogaly_features_v1.pkl (Feature names)\n",
    "   â€¢ cogaly_explainer_v1.pkl (SHAP explainer)\n",
    "   â€¢ cogaly_metrics_v1.pkl (Performance metrics)\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"âœ… Ready for production deployment!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Credits\n",
    "\n",
    "**Team:** Cogaly  \n",
    "**Hackathon:** AI 4 Alzheimer's  \n",
    "**Date:** December 2025\n",
    "\n",
    "**Disclaimer:** This model is intended for research and educational purposes. It is not approved for clinical diagnosis. Always consult qualified healthcare professionals for medical decisions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
